{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import scattertext as st\n",
    "from pprint import pprint\n",
    "import operator\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initial Spam Classification with NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filename = 'sms_data'\n",
    "# sms_df = pd.read_csv(filename, header=None, sep='\t', names=['class_sms', 'sms'])\n",
    "# sms_df['sms'] = sms_df['sms'].str.replace(r'\\d+', '')\n",
    "\n",
    "# sms_df['sms'] = sms_df['sms'].apply(lambda x: x.split())\n",
    "# sms_df['sms'] = sms_df['sms'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "# sms_df['sms'] = sms_df['sms'].apply(lambda x: ', '.join(x))\n",
    "# # print(sms_df['sms'])\n",
    "\n",
    "# X = sms_df.sms\n",
    "# y = sms_df.class_sms\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "# vect = CountVectorizer()\n",
    "# X_train_dtm = vect.fit_transform(X_train) \n",
    "# # print(vect.get_feature_names())\n",
    "# X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(X_train_dtm, y_train)\n",
    "# y_pred_class = nb.predict(X_test_dtm)  \n",
    "# false_positive = X_test[(y_test == 'spam') & (y_pred_class == 'ham')]\n",
    "# false_negative = X_test[(y_test == 'ham') & (y_pred_class == 'spam')]\n",
    "# # print(false_positive)\n",
    "# # print(false_negative)\n",
    "\n",
    "# # print(y_test.value_counts())  # examine the class distribution of the testing set (using a Pandas Series method)\n",
    "# # print(y_test.value_counts().head(1) / len(y_test)) \n",
    "# print('General Accuracy:', metrics.accuracy_score(y_test, y_pred_class)) # 0.9885\n",
    "# confusion_matrix = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "# false_positive_rate = confusion_matrix[1][0] / (confusion_matrix[1][0] + confusion_matrix[1][1])\n",
    "# print('False Positive Rate:', false_positive_rate*100) # 5.405% <-- terrible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train_tokens = vect.get_feature_names()\n",
    "# ham_token_count = nb.feature_count_[0, :]\n",
    "# spam_token_count = nb.feature_count_[1, :]\n",
    "# tokens = pd.DataFrame({'token': X_train_tokens, 'ham': ham_token_count, 'spam': spam_token_count})\n",
    "# tokens['ham'] = tokens.ham + 1\n",
    "# tokens['spam'] = tokens.spam + 1\n",
    "# tokens['ham'] = tokens.ham / nb.class_count_[0]\n",
    "# tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "# tokens['ham_ratio'] = tokens.ham / tokens.spam\n",
    "# tokens['spam_ratio'] = tokens.spam / tokens.ham\n",
    "# print(tokens.sort_values('ham_ratio', ascending=False).head(10))  # top 10 tokens predictive for 5-star\n",
    "# print(tokens.sort_values('spam_ratio', ascending=False).head(10))  # top 10 tokens predictive for 5-star\n",
    "# print(tokens.sort_values('one_star_ratio', ascending=False).head(10))  # top 10 tokens predictive for 1-star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The 2 blocks of code below produce the HTML text visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nlp = st.WhitespaceNLP.whitespace_nlp\n",
    "# corpus = st.CorpusFromPandas(sms_df, \n",
    "#                               category_col='class_sms', \n",
    "#                               text_col='sms',\n",
    "#                               nlp=nlp).build()\n",
    "# term_freq_df = corpus.get_term_freq_df()\n",
    "# term_freq_df['spam'] = corpus.get_scaled_f_scores('spam')\n",
    "# pprint(list(term_freq_df.sort_values(by='spam', ascending=False).index[:10])) # words most associated with spam\n",
    "# term_freq_df['ham'] = corpus.get_scaled_f_scores('ham')\n",
    "# pprint(list(term_freq_df.sort_values(by='ham', ascending=False).index[:10]))  # words most associated with ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# html = st.produce_scattertext_explorer(corpus,\n",
    "#           category='spam',\n",
    "#           category_name='spam',\n",
    "#           not_category_name='ham',\n",
    "#           width_in_pixels=1000,\n",
    "#           )\n",
    "# open(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Post-Naive Bayes: \n",
    "# Tokenization (playing around with data, reading RP/posts to get ideas )\n",
    "# Feature Selection (information gain)\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filename = 'sms_data'\n",
    "# sms_df = pd.read_csv(filename, header=None, sep='\t', names=['class_sms', 'sms'])\n",
    "\n",
    "# sms_df['sms'] = sms_df['sms'].str.replace(r'\\d+', '')\n",
    "# sms_df['sms'] = sms_df['sms'].str.replace(r'\\W*\\b\\w{1,2}\\b', '') # regex for replacing words of len less then 2\n",
    "# # sms_df['sms'] = sms_df['sms'].str.replace(r'\\d+', '')\n",
    "# sms_df['sms'] = sms_df['sms'].apply(lambda x: x.split())\n",
    "# sms_df['sms'] = sms_df['sms'].apply(lambda x: ','.join(x))\n",
    "# # print(sms_df)\n",
    "# # sms_df.to_csv('new_sms_data3.csv')\n",
    "# # print(len(sms_df.loc[sms_df['class_sms'] == 'spam']))\n",
    "# # sms_df.loc[sms_df['class_sms'] == 'spam'].to_csv('spam_fi2.csv')\n",
    "# # sms_df.loc[sms_df['class_sms']=='spam'].apply(lambda x: x.split())\n",
    "\n",
    "# X = sms_df.sms\n",
    "# y = sms_df.class_sms\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "# stopword = stopwords.words('english')\n",
    "# stopword.append('You')\n",
    "# stopword.append('Your')\n",
    "# vect = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b', lowercase=False, ngram_range=(1,2), stop_words=stopword)\n",
    "\n",
    "# X_train_dtm = vect.fit_transform(X_train) \n",
    "# X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(X_train_dtm, y_train)\n",
    "# y_pred_class = nb.predict(X_test_dtm)  \n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "# print(confusion_matrix)\n",
    "# false_positive_rate = confusion_matrix[1][0] / (confusion_matrix[1][0] + confusion_matrix[1][1])\n",
    "\n",
    "# print('General Accuracy:', metrics.accuracy_score(y_test, y_pred_class)) # 0.9885\n",
    "# print('False Positive Rate:', false_positive_rate*100) \n",
    "# print (classification_report(y_test, y_pred_class))\n",
    "\n",
    "# spam_token_count = nb.feature_count_[1, :] \n",
    "# X_train_tokens = vect.get_feature_names()\n",
    "# tokens = pd.DataFrame({'token':X_train_tokens, 'spam':spam_token_count})\n",
    "# tokens['spam'] = tokens.spam + 1\n",
    "# tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "\n",
    "# # print(tokens.sort_values('spam', ascending=False))  # Common Spam Value Predictor list\n",
    "\n",
    "# false_positive = X_test[(y_test == 'spam') & (y_pred_class == 'ham')]\n",
    "# false_negative = X_test[(y_test == 'ham') & (y_pred_class == 'spam')]\n",
    "# print(false_positive)\n",
    "# for i in false_positive:\n",
    "#     print(len(i))\n",
    "# print(false_negative)\n",
    "# for i in false_negative:\n",
    "#     print(len(i))\n",
    "        \n",
    "# spam_words = tokens.sort_values('spam', ascending=False)\n",
    "# spam_words = list(spam_words.token)[:70]\n",
    "# spam_wordcloud = WordCloud(width=600, height=400).generate(' '.join(spam_words))\n",
    "# plt.figure( figsize=(10,8), facecolor='k')\n",
    "# plt.imshow(spam_wordcloud)\n",
    "# plt.axis(\"off\")\n",
    "# plt.tight_layout(pad=0)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Post Information Gain, Improved False Positive from 5.48% to 5.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5773\n",
      "[[1216    8]\n",
      " [   8  212]]\n",
      "General Accuracy: 0.98891966759\n",
      "False Positive Rate: 3.63636363636\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.99      0.99      1224\n",
      "       spam       0.96      0.96      0.96       220\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1444\n",
      "\n",
      "1875    Would,you,like,see,XXX,pics,they,are,hot,they,...\n",
      "4069    TBS/PERSOLVO.,been,chasing,since,Sept,forÂ£,def...\n",
      "5674                                                  //!\n",
      "2247    babe,goten,bout?',scammers,getting,smart..Thou...\n",
      "684     sue,years,old,and,work,lapdancer,love,sex.,Tex...\n",
      "2823    ROMCAPspam,Everyone,around,should,responding,w...\n",
      "1328    balance,now,next,question:,Who,sang,'Uptown,Gi...\n",
      "5730                                                only!\n",
      "Name: sms, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def gain(data, class_label, target_attr):\n",
    "    spam_val_freq = {}\n",
    "    ham_val_freq = {}\n",
    "    total_val_freq = {}\n",
    "    subset_entropy = 0.0\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for i in range(len(data)):\n",
    "        if class_label[i] == target_attr:\n",
    "            y = data[i]\n",
    "            y = y.split(',')\n",
    "            for word in y:\n",
    "                if word in spam_val_freq:\n",
    "                    spam_val_freq[word] += 1\n",
    "                else:\n",
    "                    spam_val_freq[word] = 1   \n",
    "                    \n",
    "        if class_label[i] != target_attr:\n",
    "            y = data[i]\n",
    "            y = y.split(',')\n",
    "            for word in y:\n",
    "                if word in ham_val_freq:\n",
    "                    ham_val_freq[word] += 1\n",
    "                else:\n",
    "                    ham_val_freq[word] = 1 \n",
    "    \n",
    "    for word in spam_val_freq:\n",
    "        if word in ham_val_freq:\n",
    "            total_val_freq[word] = spam_val_freq[word] + ham_val_freq[word]\n",
    "        else:\n",
    "            total_val_freq[word] = 1\n",
    "    \n",
    "    prob_ham = len(sms_df.loc[sms_df['class_sms'] == 'spam']) / len(X)\n",
    "    prob_spam = len(sms_df.loc[sms_df['class_sms'] == 'ham']) / len(X)\n",
    "    total_entropy = -(prob_ham) * math.log(prob_ham, 2) - prob_spam * math.log(prob_spam, 2)\n",
    "\n",
    "    # calculation of conditional entropy \n",
    "    entropy_dict = {}\n",
    "    for word in spam_val_freq:\n",
    "        spam_val_prob = spam_val_freq[word] / total_val_freq[word]\n",
    "#         ham_val_prob = ham_val_freq[word] / total_val_freq[word]\n",
    "        entropy = -(spam_val_prob)*math.log(spam_val_prob, 2) \n",
    "#         entropy = -(spam_val_prob)*math.log(spam_val_prob, 2) - ham_val_prob*math.log(ham_val_prob, 2)\n",
    "        word_entropy = total_entropy - entropy\n",
    "        if len(word) > 1:   \n",
    "#             regex = re.compile('[^a-zA-Z]')\n",
    "#             word = regex.sub('', word)\n",
    "            entropy_dict[word] = word_entropy\n",
    "    entropy_dict = sorted(entropy_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return entropy_dict[:200]\n",
    "    \n",
    "    \n",
    "filename = 'sms_data'\n",
    "sms_df = pd.read_csv(filename, header=None, sep='\t', names=['class_sms', 'sms'])\n",
    "\n",
    "sms_df['sms'] = sms_df['sms'].str.replace(r'\\d+', '')\n",
    "sms_df['sms'] = sms_df['sms'].str.replace(r'\\W*\\b\\w{1,2}\\b', '') # regex for replacing words of len less then 2\n",
    "sms_df['sms'] = sms_df['sms'].apply(lambda x: x.split())\n",
    "sms_df['sms'] = sms_df['sms'].apply(lambda x: ','.join(x))\n",
    "\n",
    "X = sms_df.sms\n",
    "y = sms_df.class_sms\n",
    "\n",
    "entropy_dict = gain(X, y, 'spam')\n",
    "# entropy_dict = [list(i).append('spam') for i in entropy_dict]\n",
    "new_li = []\n",
    "for i in entropy_dict:\n",
    "    i = list(i)\n",
    "    i.append('spam')\n",
    "    new_li.append(i)\n",
    "new_df = pd.DataFrame(new_li, columns=['sms', 'entropy_level', 'class_sms'])\n",
    "new_df1 = new_df[['sms', 'class_sms']]\n",
    "sms_df = sms_df.append(new_df1, ignore_index=True)\n",
    "\n",
    "X = sms_df.sms\n",
    "y = sms_df.class_sms\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "stopword = stopwords.words('english')\n",
    "stopword.append('You')\n",
    "stopword.append('Your')\n",
    "vect = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b', lowercase=False, ngram_range=(1,2), stop_words=stopword)\n",
    "\n",
    "X_train_dtm = vect.fit_transform(X_train) \n",
    "X_test_dtm = vect.transform(X_test)    \n",
    "entropy_dict = gain(X, y, 'spam')\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "y_pred_class = nb.predict(X_test_dtm)  \n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(confusion_matrix)\n",
    "false_positive_rate = confusion_matrix[1][0] / (confusion_matrix[1][0] + confusion_matrix[1][1])\n",
    "\n",
    "print('General Accuracy:', metrics.accuracy_score(y_test, y_pred_class)) # 0.9885\n",
    "print('False Positive Rate:', false_positive_rate*100) \n",
    "print (classification_report(y_test, y_pred_class))\n",
    "\n",
    "spam_token_count = nb.feature_count_[1, :] \n",
    "X_train_tokens = vect.get_feature_names()\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'spam':spam_token_count})\n",
    "tokens['spam'] = tokens.spam + 1\n",
    "tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "\n",
    "# print(tokens.sort_values('spam', ascending=False))  # Common Spam Value Predictor list\n",
    "\n",
    "false_positive = X_test[(y_test == 'spam') & (y_pred_class == 'ham')]\n",
    "false_negative = X_test[(y_test == 'ham') & (y_pred_class == 'spam')]\n",
    "print(false_positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5026, 4179]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a59220ef2b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# entropy_dict = gain(X, y, 'spam')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_dtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0my_pred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_dtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rahul/anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \"\"\"\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rahul/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Rahul/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5026, 4179]"
     ]
    }
   ],
   "source": [
    "## Method 2 of IG:\n",
    "\n",
    "def calculate_weight():\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculates the entropy of the given data set for the target attribute.\n",
    "# def entropy(data, class_label, target_attr): \n",
    "#     val_freq = {}\n",
    "#     data_entropy = 0.0\n",
    "\n",
    "#     # Calculate the frequency of each of the values in the target attribute\n",
    "#     for i in range(len(data)):\n",
    "#         if class_label[i] == target_attr:\n",
    "#             y = data[i]\n",
    "#             y = y.split(',')\n",
    "#             for word in y:\n",
    "#                 if word in val_freq:\n",
    "#                     val_freq[word] += 1\n",
    "#                 else:\n",
    "#                     val_freq[word] = 1    \n",
    "# #     print(val_freq)\n",
    "        \n",
    "# #     # Calculate the entropy of the data for the target attribute\n",
    "#     for freq in val_freq.values():\n",
    "#         data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    "\n",
    "#     return data_entropy\n",
    "\n",
    "# print(entropy(X, y, 'spam'))\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculating the conditional entropy \n",
    "#     entropy_list = []\n",
    "#     for val in spam_val_freq.keys():\n",
    "#         if val in ham_val_freq.keys():\n",
    "#             ham_val_prob = ham_val_freq[val] / sum(ham_val_freq.values())            \n",
    "#             spam_val_prob = spam_val_freq[val] / sum(spam_val_freq.values())\n",
    "#             entropy = -(spam_val_prob)*math.log(spam_val_prob, 2) - ham_val_prob*math.log(ham_val_prob, 2)\n",
    "#             entropy_list.append([val, entropy])\n",
    "# #     print(entropy_list)\n",
    "    \n",
    "#     prob_ham = len(sms_df.loc[sms_df['class_sms'] == 'spam']) / len(X)\n",
    "#     prob_spam = len(sms_df.loc[sms_df['class_sms'] == 'ham']) / len(X)\n",
    "#     total_entropy = -(prob_ham) * math.log(prob_ham, 2) - prob_spam * math.log(prob_spam, 2)\n",
    "#     information_gain_list = []\n",
    "#     for li in entropy_list:\n",
    "#         information_gain = total_entropy - li[-1]\n",
    "#         information_gain_list.append([li[0], information_gain])\n",
    "#     print(information_gain_list)\n",
    "    \n",
    "#     print(len(sms_df.loc[sms_df['class_sms'] == 'spam']))\n",
    "#     print(len(sms_df.loc[sms_df['class_sms'] == 'ham']))\n",
    "\n",
    "\n",
    "# spam_val_freq = {}\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     if y[i] == 'spam':\n",
    "#         w = X[i]\n",
    "#         w = w.split(',')\n",
    "#         for word in w:\n",
    "#             if word in spam_val_freq:\n",
    "#                 spam_val_freq[word] += 1\n",
    "#             else:\n",
    "#                 spam_val_freq[word] = 1   \n",
    "\n",
    "# sum_value = sum(spam_val_freq.values())\n",
    "# prob_dict = {}\n",
    "# for i in spam_val_freq:\n",
    "#     prob = spam_val_freq[i] / sum_value\n",
    "#     prob_dict[i] = prob\n",
    "# print(prob_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nlp = st.WhitespaceNLP.whitespace_nlp\n",
    "# corpus = st.CorpusFromPandas(sms_df, \n",
    "#                               category_col='class_sms', \n",
    "#                               text_col='sms',\n",
    "#                               nlp=nlp).build()\n",
    "# term_freq_df = corpus.get_term_freq_df()\n",
    "# term_freq_df['spam'] = corpus.get_scaled_f_scores('spam')\n",
    "# pprint(list(term_freq_df.sort_values(by='spam', ascending=False).index[:10])) # words most associated with spam\n",
    "# term_freq_df['ham'] = corpus.get_scaled_f_scores('ham')\n",
    "# pprint(list(term_freq_df.sort_values(by='ham', ascending=False).index[:10]))  # words most associated with ham\n",
    "# html = st.produce_scattertext_explorer(corpus,\n",
    "#           category='spam',\n",
    "#           category_name='spam',\n",
    "#           not_category_name='ham',\n",
    "#           width_in_pixels=1000,\n",
    "#           )\n",
    "# open(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
