{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import scattertext as st\n",
    "from pprint import pprint\n",
    "import operator\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initial Spam Classification with NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filename = 'sms_data'\n",
    "# sms_df = pd.read_csv(filename, header=None, sep='\t', names=['class_sms', 'sms'])\n",
    "# sms_df['sms'] = sms_df['sms'].str.replace(r'\\d+', '')\n",
    "\n",
    "# sms_df['sms'] = sms_df['sms'].apply(lambda x: x.split())\n",
    "# sms_df['sms'] = sms_df['sms'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "# sms_df['sms'] = sms_df['sms'].apply(lambda x: ', '.join(x))\n",
    "# # print(sms_df['sms'])\n",
    "\n",
    "# X = sms_df.sms\n",
    "# y = sms_df.class_sms\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "# vect = CountVectorizer()\n",
    "# X_train_dtm = vect.fit_transform(X_train) \n",
    "# # print(vect.get_feature_names())\n",
    "# X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(X_train_dtm, y_train)\n",
    "# y_pred_class = nb.predict(X_test_dtm)  \n",
    "# false_positive = X_test[(y_test == 'spam') & (y_pred_class == 'ham')]\n",
    "# false_negative = X_test[(y_test == 'ham') & (y_pred_class == 'spam')]\n",
    "# # print(false_positive)\n",
    "# # print(false_negative)\n",
    "\n",
    "# # print(y_test.value_counts())  # examine the class distribution of the testing set (using a Pandas Series method)\n",
    "# # print(y_test.value_counts().head(1) / len(y_test)) \n",
    "# print('General Accuracy:', metrics.accuracy_score(y_test, y_pred_class)) # 0.9885\n",
    "# confusion_matrix = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "# false_positive_rate = confusion_matrix[1][0] / (confusion_matrix[1][0] + confusion_matrix[1][1])\n",
    "# print('False Positive Rate:', false_positive_rate*100) # 5.405% <-- terrible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train_tokens = vect.get_feature_names()\n",
    "# ham_token_count = nb.feature_count_[0, :]\n",
    "# spam_token_count = nb.feature_count_[1, :]\n",
    "# tokens = pd.DataFrame({'token': X_train_tokens, 'ham': ham_token_count, 'spam': spam_token_count})\n",
    "# tokens['ham'] = tokens.ham + 1\n",
    "# tokens['spam'] = tokens.spam + 1\n",
    "# tokens['ham'] = tokens.ham / nb.class_count_[0]\n",
    "# tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "# tokens['ham_ratio'] = tokens.ham / tokens.spam\n",
    "# tokens['spam_ratio'] = tokens.spam / tokens.ham\n",
    "# print(tokens.sort_values('ham_ratio', ascending=False).head(10))  # top 10 tokens predictive for 5-star\n",
    "# print(tokens.sort_values('spam_ratio', ascending=False).head(10))  # top 10 tokens predictive for 5-star\n",
    "# print(tokens.sort_values('one_star_ratio', ascending=False).head(10))  # top 10 tokens predictive for 1-star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The 2 blocks of code below produce the HTML text visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nlp = st.WhitespaceNLP.whitespace_nlp\n",
    "# corpus = st.CorpusFromPandas(sms_df, \n",
    "#                               category_col='class_sms', \n",
    "#                               text_col='sms',\n",
    "#                               nlp=nlp).build()\n",
    "# term_freq_df = corpus.get_term_freq_df()\n",
    "# term_freq_df['spam'] = corpus.get_scaled_f_scores('spam')\n",
    "# pprint(list(term_freq_df.sort_values(by='spam', ascending=False).index[:10])) # words most associated with spam\n",
    "# term_freq_df['ham'] = corpus.get_scaled_f_scores('ham')\n",
    "# pprint(list(term_freq_df.sort_values(by='ham', ascending=False).index[:10]))  # words most associated with ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# html = st.produce_scattertext_explorer(corpus,\n",
    "#           category='spam',\n",
    "#           category_name='spam',\n",
    "#           not_category_name='ham',\n",
    "#           width_in_pixels=1000,\n",
    "#           )\n",
    "# open(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Post-Naive Bayes: \n",
    "# Tokenization (playing around with data, reading RP/posts to get ideas )\n",
    "# Feature Selection (information gain)\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1203    7]\n",
      " [  10  174]]\n",
      "General Accuracy: 0.987804878049\n",
      "False Positive Rate: 5.4347826087\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.99      0.99      0.99      1210\n",
      "       spam       0.96      0.95      0.95       184\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1394\n",
      "\n",
      "1875    Would,you,like,see,XXX,pics,they,are,hot,they,...\n",
      "684     sue,years,old,and,work,lapdancer,love,sex.,Tex...\n",
      "1328    balance,now,next,question:,Who,sang,'Uptown,Gi...\n",
      "3419    LIFE,has,never,been,this,much,fun,and,great,un...\n",
      "4949    this,Amy,will,sending,you,free,phone,number,co...\n",
      "4069    TBS/PERSOLVO.,been,chasing,since,Sept,forÂ£,def...\n",
      "2823    ROMCAPspam,Everyone,around,should,responding,w...\n",
      "191     Are,you,unique,enough?,Find,out,from,August.,w...\n",
      "4256    Block,Breaker,now,comes,deluxe,format,with,new...\n",
      "4821    Check,Out,Choose,Your,Babe,Videos,@,sms.shsex....\n",
      "Name: sms, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filename = 'sms_data'\n",
    "sms_df = pd.read_csv(filename, header=None, sep='\t', names=['class_sms', 'sms'])\n",
    "\n",
    "sms_df['sms'] = sms_df['sms'].str.replace(r'\\d+', '')\n",
    "sms_df['sms'] = sms_df['sms'].str.replace(r'\\W*\\b\\w{1,2}\\b', '') # regex for replacing words of len less then 2\n",
    "# sms_df['sms'] = sms_df['sms'].str.replace(r'\\d+', '')\n",
    "sms_df['sms'] = sms_df['sms'].apply(lambda x: x.split())\n",
    "sms_df['sms'] = sms_df['sms'].apply(lambda x: ','.join(x))\n",
    "# print(sms_df)\n",
    "# sms_df.to_csv('new_sms_data3.csv')\n",
    "# print(len(sms_df.loc[sms_df['class_sms'] == 'spam']))\n",
    "# sms_df.loc[sms_df['class_sms'] == 'spam'].to_csv('spam_fi2.csv')\n",
    "# sms_df.loc[sms_df['class_sms']=='spam'].apply(lambda x: x.split())\n",
    "\n",
    "X = sms_df.sms\n",
    "y = sms_df.class_sms\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "stopword = stopwords.words('english')\n",
    "stopword.append('You')\n",
    "stopword.append('Your')\n",
    "vect = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b', lowercase=False, ngram_range=(1,2), stop_words=stopword)\n",
    "\n",
    "X_train_dtm = vect.fit_transform(X_train) \n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)  \n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(confusion_matrix)\n",
    "false_positive_rate = confusion_matrix[1][0] / (confusion_matrix[1][0] + confusion_matrix[1][1])\n",
    "\n",
    "print('General Accuracy:', metrics.accuracy_score(y_test, y_pred_class)) # 0.9885\n",
    "print('False Positive Rate:', false_positive_rate*100) \n",
    "print (classification_report(y_test, y_pred_class))\n",
    "\n",
    "spam_token_count = nb.feature_count_[1, :] \n",
    "X_train_tokens = vect.get_feature_names()\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'spam':spam_token_count})\n",
    "tokens['spam'] = tokens.spam + 1\n",
    "tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "\n",
    "# print(tokens.sort_values('spam', ascending=False))  # Common Spam Value Predictor list\n",
    "\n",
    "false_positive = X_test[(y_test == 'spam') & (y_pred_class == 'ham')]\n",
    "false_negative = X_test[(y_test == 'ham') & (y_pred_class == 'spam')]\n",
    "print(false_positive)\n",
    "# for i in false_positive:\n",
    "#     print(len(i))\n",
    "# print(false_negative)\n",
    "# for i in false_negative:\n",
    "#     print(len(i))\n",
    "        \n",
    "spam_words = tokens.sort_values('spam', ascending=False)\n",
    "spam_words = list(spam_words.token)[:70]\n",
    "spam_wordcloud = WordCloud(width=600, height=400).generate(' '.join(spam_words))\n",
    "plt.figure( figsize=(10,8), facecolor='k')\n",
    "plt.imshow(spam_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates the entropy of the given data set for the target attribute.\n",
    "def entropy(data, class_label, target_attr): \n",
    "    val_freq = {}\n",
    "    data_entropy = 0.0\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for i in range(len(data)):\n",
    "        if class_label[i] == target_attr:\n",
    "            y = data[i]\n",
    "            y = y.split(',')\n",
    "            for word in y:\n",
    "                if word in val_freq:\n",
    "                    val_freq[word] += 1\n",
    "                else:\n",
    "                    val_freq[word] = 1    \n",
    "#     print(val_freq)\n",
    "        \n",
    "#     # Calculate the entropy of the data for the target attribute\n",
    "    for freq in val_freq.values():\n",
    "        data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    "\n",
    "    return data_entropy\n",
    "\n",
    "# print(entropy(X, y, 'spam'))\n",
    "\n",
    "def gain(data, class_label, target_attr):\n",
    "    spam_val_freq = {}\n",
    "    ham_val_freq = {}\n",
    "    subset_entropy = 0.0\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for i in range(len(data)):\n",
    "        if class_label[i] == target_attr:\n",
    "            y = data[i]\n",
    "            y = y.split(',')\n",
    "            for word in y:\n",
    "                if word in spam_val_freq:\n",
    "                    spam_val_freq[word] += 1\n",
    "                else:\n",
    "                    spam_val_freq[word] = 1   \n",
    "                    \n",
    "        if class_label[i] != target_attr:\n",
    "            y = data[i]\n",
    "            y = y.split(',')\n",
    "            for word in y:\n",
    "                if word in ham_val_freq:\n",
    "                    ham_val_freq[word] += 1\n",
    "                else:\n",
    "                    ham_val_freq[word] = 1 \n",
    "\n",
    "    # Calculating the conditional entropy \n",
    "    entropy_list = []\n",
    "    for val in spam_val_freq.keys():\n",
    "        if val in ham_val_freq.keys():\n",
    "            ham_val_prob = ham_val_freq[val] / sum(ham_val_freq.values())            \n",
    "            spam_val_prob = spam_val_freq[val] / sum(spam_val_freq.values())\n",
    "            entropy = -(spam_val_prob * math.log(spam_val_prob, 2)) + (-(ham_val_prob * math.log(ham_val_prob, 2))) \n",
    "            entropy_list.append([val, entropy])\n",
    "#     print(entropy_list)\n",
    "    \n",
    "\n",
    "    prob_ham = len(sms_df.loc[sms_df['class_sms'] == 'spam']) / len(X)\n",
    "    prob_spam = len(sms_df.loc[sms_df['class_sms'] == 'ham']) / len(X)\n",
    "    total_entropy = -prob_ham * math.log(prob_ham, 2) - prob_spam * math.log(prob_spam, 2)\n",
    "    information_gain_list = []\n",
    "    for li in entropy_list:\n",
    "        information_gain = total_entropy - li[-1]\n",
    "        information_gain_list.append([li[0], information_gain])\n",
    "    print(information_gain_list)\n",
    "#     print(len(sms_df.loc[sms_df['class_sms'] == 'spam']))\n",
    "#     print(len(sms_df.loc[sms_df['class_sms'] == 'ham']))\n",
    "\n",
    "# gain(X, y, 'spam')\n",
    "\n",
    "# spam_val_freq = {}\n",
    "\n",
    "# for i in range(len(X)):\n",
    "#     if y[i] == 'spam':\n",
    "#         w = X[i]\n",
    "#         w = w.split(',')\n",
    "#         for word in w:\n",
    "#             if word in spam_val_freq:\n",
    "#                 spam_val_freq[word] += 1\n",
    "#             else:\n",
    "#                 spam_val_freq[word] = 1   \n",
    "\n",
    "# sum_value = sum(spam_val_freq.values())\n",
    "# prob_dict = {}\n",
    "# for i in spam_val_freq:\n",
    "#     prob = spam_val_freq[i] / sum_value\n",
    "#     prob_dict[i] = prob\n",
    "# print(prob_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nlp = st.WhitespaceNLP.whitespace_nlp\n",
    "# corpus = st.CorpusFromPandas(sms_df, \n",
    "#                               category_col='class_sms', \n",
    "#                               text_col='sms',\n",
    "#                               nlp=nlp).build()\n",
    "# term_freq_df = corpus.get_term_freq_df()\n",
    "# term_freq_df['spam'] = corpus.get_scaled_f_scores('spam')\n",
    "# pprint(list(term_freq_df.sort_values(by='spam', ascending=False).index[:10])) # words most associated with spam\n",
    "# term_freq_df['ham'] = corpus.get_scaled_f_scores('ham')\n",
    "# pprint(list(term_freq_df.sort_values(by='ham', ascending=False).index[:10]))  # words most associated with ham\n",
    "# html = st.produce_scattertext_explorer(corpus,\n",
    "#           category='spam',\n",
    "#           category_name='spam',\n",
    "#           not_category_name='ham',\n",
    "#           width_in_pixels=1000,\n",
    "#           )\n",
    "# open(\"Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
